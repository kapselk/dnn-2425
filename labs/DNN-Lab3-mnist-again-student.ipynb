{"cells":[{"cell_type":"markdown","metadata":{"id":"84VetyCaGLyR"},"source":["<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n","\n","AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n","<hr>\n","\n","<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n","\n","<center>\n","Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n","Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n","Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n","Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n","    </center>"]},{"cell_type":"markdown","metadata":{"id":"ziZ9i7tXbO1T"},"source":["In this lab, you will implement some of the techniques discussed in the lecture.\n","\n","Below you are given a solution to the previous scenario. Note that it has two serious drawbacks:\n"," * The output predictions do not sum up to one (i.e. it does not return a distribution) even though the images always contain exactly one digit.\n"," * It uses MSE coupled with output sigmoid which can lead to saturation and slow convergence\n","\n","**Task 0.** Implement a numerically stable version of softmax.\n","\n","**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block and not even compute the gradient over the softmax values.\n","\n","**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n","\n","**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n","\n","**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n","\n","The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GS-RGKqd1qoI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730984647434,"user_tz":-60,"elapsed":18789,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}},"outputId":"9f6a1f7a-383b-46fe-f69f-ca4df097786e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["!pip install tqdm\n","!pip install pandas"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"P22HqX9AbO1a","executionInfo":{"status":"ok","timestamp":1730984672320,"user_tz":-60,"elapsed":24891,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}}},"outputs":[],"source":["import random\n","import numpy as np\n","from torchvision import datasets, transforms\n","from typing import List, Any, Tuple, Optional, Callable, Dict\n","from numpy.typing import NDArray\n","import json\n","from tqdm import tqdm\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"N9jGPaZhbO2B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730984675068,"user_tz":-60,"elapsed":2757,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}},"outputId":"54e7cb12-6570-40b8-ba1c-b25d8ed242b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 97.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 17.0MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 48.0MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 7.66MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Let's read the mnist dataset\n","\n","\n","def load_mnist(path: str = \".\"):\n","    train_set = datasets.MNIST(path, train=True, download=True)\n","    x_train = train_set.data.numpy()\n","    _y_train = train_set.targets.numpy()\n","\n","    test_set = datasets.MNIST(path, train=False, download=True)\n","    x_test = test_set.data.numpy()\n","    _y_test = test_set.targets.numpy()\n","\n","    x_train = x_train.reshape((x_train.shape[0], 28 * 28)) / 255.0\n","    x_test = x_test.reshape((x_test.shape[0], 28 * 28)) / 255.0\n","\n","    y_train = np.zeros((_y_train.shape[0], 10))\n","    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n","\n","    y_test = np.zeros((_y_test.shape[0], 10))\n","    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","\n","(x_train, y_train), (x_test, y_test) = load_mnist()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"w3gAyqw4bO1p","executionInfo":{"status":"ok","timestamp":1730984675069,"user_tz":-60,"elapsed":5,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}}},"outputs":[],"source":["def sigmoid(z: NDArray[float]):\n","    return 1.0 / (1.0 + np.exp(-z))\n","\n","\n","def sigmoid_prime(z: NDArray[float]):\n","    # Derivative of the sigmoid\n","    return sigmoid(z) * (1 - sigmoid(z))"]},{"cell_type":"markdown","metadata":{"id":"W8L6nDcU1qoM"},"source":["## Warm-Up\n","Implement a numerically stable version of softmax.  \n","In general, softmax is defined as  \n","$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n","However, taking $e^{1000000}$ can result in NaN.  \n","Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n","\n","Hint: <sub><sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub></sub>"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"SEYH4a8e1qoN","executionInfo":{"status":"ok","timestamp":1730984726045,"user_tz":-60,"elapsed":316,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}}},"outputs":[],"source":["def unstable_softmax(x: NDArray[float], axis: int = -1):\n","    e = np.exp(x)\n","    return e / np.sum(e, axis=axis, keepdims=True)\n","\n","\n","def stable_softmax(x: NDArray[float], axis: int = -1):\n","    ## TODO\n","    ###{\n","    max_x = np.max(x, axis=axis, keepdims=True)\n","    e = np.exp(x - max_x)\n","    return e / np.sum(e, axis=axis, keepdims=True)\n","    ###}\n","\n","\n","### TESTS ###\n","def test_one(x: NDArray[float], y: NDArray[float], proc_fn: Callable[[NDArray[float]], NDArray[float]]):\n","    r = stable_softmax(x)\n","    assert r.shape == x.shape\n","    r = proc_fn(r)\n","    assert r.shape == y.shape\n","    diff = np.mean(np.abs(r - y))\n","    assert diff <= 1e-5, r\n","\n","\n","x1 = np.random.rand(100, 32).astype(np.float64)\n","test_one(x1, np.ones(100), lambda x: x.sum(-1))\n","test_one(x1, unstable_softmax(x1), lambda x: x)\n","\n","\n","x2 = np.ones((100, 32), dtype=np.float64) * 1e6\n","test_one(x2, np.ones_like(x2) / x2.shape[-1], lambda x: x)\n","### TESTS END ###"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"FgEA2XRRbO2X","colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"status":"ok","timestamp":1730984990505,"user_tz":-60,"elapsed":261628,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}},"outputId":"6669b9fb-db90-472a-ebee-ef16f87fe7e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.001\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.0693: 100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n","Epoch: 9, Accuracy: 0.0918: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n","Epoch: 9, Accuracy: 0.1399: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.01\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.1315: 100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n","Epoch: 9, Accuracy: 0.1755: 100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n","Epoch: 9, Accuracy: 0.2162: 100%|██████████| 10/10 [00:15<00:00,  1.57s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.1\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.489: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n","Epoch: 9, Accuracy: 0.5077: 100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n","Epoch: 9, Accuracy: 0.6276: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 1.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.881: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]\n","Epoch: 9, Accuracy: 0.8882: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n","Epoch: 9, Accuracy: 0.8938: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 10.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.9349: 100%|██████████| 10/10 [00:17<00:00,  1.70s/it]\n","Epoch: 9, Accuracy: 0.9311: 100%|██████████| 10/10 [00:17<00:00,  1.71s/it]\n","Epoch: 9, Accuracy: 0.9355: 100%|██████████| 10/10 [00:16<00:00,  1.60s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["  MODEL      LR       ACC_STD\n","4  Base  10.000  93.4% +- 0.2\n","3  Base   1.000  88.8% +- 0.5\n","2  Base   0.100  54.1% +- 6.1\n","1  Base   0.010  17.4% +- 3.5\n","0  Base   0.001  10.0% +- 2.9"],"text/html":["\n","  <div id=\"df-c3286d58-aebe-459b-b888-cda283458158\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MODEL</th>\n","      <th>LR</th>\n","      <th>ACC_STD</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>Base</td>\n","      <td>10.000</td>\n","      <td>93.4% +- 0.2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Base</td>\n","      <td>1.000</td>\n","      <td>88.8% +- 0.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Base</td>\n","      <td>0.100</td>\n","      <td>54.1% +- 6.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Base</td>\n","      <td>0.010</td>\n","      <td>17.4% +- 3.5</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Base</td>\n","      <td>0.001</td>\n","      <td>10.0% +- 2.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3286d58-aebe-459b-b888-cda283458158')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c3286d58-aebe-459b-b888-cda283458158 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c3286d58-aebe-459b-b888-cda283458158');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-aba8f702-c0d8-4606-8256-6bd3afeeefa2\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aba8f702-c0d8-4606-8256-6bd3afeeefa2')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-aba8f702-c0d8-4606-8256-6bd3afeeefa2 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"MODEL\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Base\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.368044093184042,\n        \"min\": 0.001,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ACC_STD\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"88.8% +- 0.5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}],"source":["class Network(object):\n","    def __init__(self, sizes: List[int]):\n","        # initialize biases and weights with random normal distr.\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y) for y in sizes[1:]]\n","        self.weights = [np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n","\n","    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n","        # Run the network on a single case\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(a @ w + b)\n","\n","        return a\n","\n","    def update_mini_batch(\n","        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n","    ) -> None:\n","        # Update network weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","\n","        nabla_b, nabla_w = self.backprop(x_mini_batch, y_mini_batch)\n","\n","        self.weights = [w - eta * nw for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b - eta * nb for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(\n","        self, x: NDArray[float], y: NDArray[float]\n","    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","\n","        assert len(x.shape) == 2  # batch, features\n","        assert len(y.shape) == 2  # batch, classes\n","        assert x.shape[0] == y.shape[0]\n","\n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = []\n","        delta_nabla_w = []\n","\n","        # Then go forward remembering each layer input and value\n","        # before sigmoid activation\n","        layer_input = []\n","        before_act = []\n","        for w, b in zip(self.weights, self.biases):\n","            layer_input.append(x)\n","            x = x @ w + b\n","            before_act.append(x)\n","            x = sigmoid(x)\n","\n","        # Now go backward from the final cost applying backpropagation\n","        diff = self.cost_derivative(output_activations=x, y=y)\n","        for linp, bef_act, w, b in reversed(\n","            list(zip(layer_input, before_act, self.weights, self.biases))\n","        ):\n","            diff = sigmoid_prime(bef_act) * diff\n","            delta_nabla_w.append(linp.T @ diff)\n","            delta_nabla_b.append(np.sum(diff, axis=0))\n","            diff = diff @ w.T\n","\n","        delta_nabla_w = reversed(delta_nabla_w)\n","        delta_nabla_b = reversed(delta_nabla_b)\n","\n","        # Check shapes\n","        delta_nabla_b = list(delta_nabla_b)\n","        delta_nabla_w = list(delta_nabla_w)\n","        assert len(delta_nabla_b) == len(self.biases), (\n","            len(delta_nabla_b),\n","            len(self.biases),\n","        )\n","        assert len(delta_nabla_w) == len(self.weights), (\n","            len(delta_nabla_w),\n","            len(self.weights),\n","        )\n","        for lid in range(len(self.weights)):\n","            assert delta_nabla_b[lid].shape == self.biases[lid].shape, (\n","                delta_nabla_b[lid].shape,\n","                self.biases[lid].shape,\n","            )\n","            assert delta_nabla_w[lid].shape == self.weights[lid].shape, (\n","                delta_nabla_w[lid].shape,\n","                self.weights[lid].shape,\n","            )\n","\n","        return delta_nabla_b, delta_nabla_w\n","\n","    def evaluate(\n","        self, x_test_data: NDArray[float], y_test_data: NDArray[float]\n","    ) -> float:\n","        # Count the number of correct answers for test_data\n","        test_results = [\n","            (\n","                np.argmax(self.feedforward(x_test_data[i].reshape(1, 784)), axis=-1),\n","                np.argmax(y_test_data[i], axis=-1),\n","            )\n","            for i in range(len(x_test_data))\n","        ]\n","        # return accuracy\n","        return np.mean([int((x == y).item()) for (x, y) in test_results]).item()\n","\n","    def cost_derivative(\n","        self, output_activations: NDArray[float], y: NDArray[float]\n","    ) -> NDArray[float]:\n","        assert output_activations.shape == y.shape, (output_activations.shape, y.shape)\n","        return (output_activations - y) / len(y)  # moved here from update_mini_batch\n","\n","    def optimize(\n","        self,\n","        training_data: Tuple[NDArray[float], NDArray[float]],\n","        epochs: int,\n","        mini_batch_size: int,\n","        eta: float,\n","        test_data: Optional[Tuple[NDArray[float], NDArray[float]]] = None,\n","    ) -> None:\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        epoch_bar = tqdm(range(epochs), desc=\"Epoch\")\n","        for j in epoch_bar:\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[\n","                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n","                ]\n","                y_mini_batch = y_train[\n","                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n","                ]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                epoch_bar.set_description_str(\n","                    \"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test))\n","                )\n","            else:\n","                epoch_bar.set_description_str(\"Epoch: {0}\".format(j))\n","\n","        return self.evaluate(x_test, y_test)\n","\n","\n","def evaluate_model(\n","    model_name: str,\n","    model_constructor: Callable[[List[int]], Network],\n","    result_storage: Dict[str, List[Any]],\n","    layers: List[int] = [784, 30, 10],\n","):\n","    # Remove logs from the previous evaluation of the same model\n","    result_storage[\"MODEL\"] = [\n","        m for m in result_storage.get(\"MODEL\", []) if m != model_name\n","    ]\n","    result_storage[\"LR\"] = [\n","        lr\n","        for m, lr in zip(result_storage.get(\"MODEL\", []), result_storage.get(\"LR\", []))\n","        if m != model_name\n","    ]\n","    result_storage[\"ACC_STD\"] = [\n","        acc_std\n","        for m, acc_std in zip(\n","            result_storage.get(\"MODEL\", []), result_storage.get(\"ACC_STD\", [])\n","        )\n","        if m != model_name\n","    ]\n","    result_storage[\"ACC\"] = [\n","        acc\n","        for m, acc in zip(\n","            result_storage.get(\"MODEL\", []), result_storage.get(\"ACC\", [])\n","        )\n","        if m != model_name\n","    ]\n","\n","    for lr in [0.001, 0.01, 0.1, 1.0, 10.0]:\n","\n","        print(f\"Checking with lr = {lr}\")\n","        np.random.seed(42)\n","        accuracy_list = []\n","        for i in range(3):\n","            network = model_constructor(layers)\n","            accuracy = network.optimize(\n","                (x_train, y_train),\n","                epochs=10,\n","                mini_batch_size=100,\n","                eta=lr,\n","                test_data=(x_test, y_test),\n","            )\n","            accuracy_list.append(accuracy)\n","\n","        result_storage[\"MODEL\"].append(model_name)\n","        result_storage[\"LR\"].append(lr)\n","        result_storage[\"ACC_STD\"].append(\n","            f\"{np.mean(accuracy_list) * 100:2.1f}% +- {np.std(accuracy_list) * 100:.1f}\"\n","        )\n","\n","        result_storage[\"ACC\"].append(np.mean(accuracy_list))\n","\n","    df = pd.DataFrame(result_storage).sort_values(\"ACC\", ascending=False)\n","    df = df[[c for c in df.columns if c != \"ACC\"]]\n","    return df\n","\n","\n","RESULTS = {}\n","evaluate_model(\n","    model_name=\"Base\",\n","    model_constructor=lambda x: Network(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ioyTp-7d1qoP"},"source":["## Task 1\n","Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence.   \n","Hint: When implementing backprop it might be easier to consider these two functions as a single block and not even compute the gradient over the softmax values.  \n","If you have problems, please see Appendix A.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"DyBsznpL1qoQ","colab":{"base_uri":"https://localhost:8080/","height":710},"executionInfo":{"status":"ok","timestamp":1730985565629,"user_tz":-60,"elapsed":287616,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}},"outputId":"3aa6da64-b8b4-4723-eb5e-17d6df6f4804"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.001\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.1574: 100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n","Epoch: 9, Accuracy: 0.1934: 100%|██████████| 10/10 [00:18<00:00,  1.82s/it]\n","Epoch: 9, Accuracy: 0.2256: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.01\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.6627: 100%|██████████| 10/10 [00:18<00:00,  1.85s/it]\n","Epoch: 9, Accuracy: 0.6577: 100%|██████████| 10/10 [00:18<00:00,  1.87s/it]\n","Epoch: 9, Accuracy: 0.6885: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.1\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.8665: 100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n","Epoch: 9, Accuracy: 0.8733: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n","Epoch: 9, Accuracy: 0.8743: 100%|██████████| 10/10 [00:19<00:00,  1.95s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 1.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.9346: 100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n","Epoch: 9, Accuracy: 0.9286: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n","Epoch: 9, Accuracy: 0.9321: 100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 10.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.9422: 100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n","Epoch: 9, Accuracy: 0.943: 100%|██████████| 10/10 [00:19<00:00,  1.94s/it]\n","Epoch: 9, Accuracy: 0.9397: 100%|██████████| 10/10 [00:17<00:00,  1.76s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["     MODEL      LR       ACC_STD\n","9  SoftMax  10.000  94.2% +- 0.1\n","4     Base  10.000  93.4% +- 0.2\n","8  SoftMax   1.000  93.2% +- 0.2\n","3     Base   1.000  88.8% +- 0.5\n","7  SoftMax   0.100  87.1% +- 0.3\n","6  SoftMax   0.010  67.0% +- 1.3\n","2     Base   0.100  54.1% +- 6.1\n","5  SoftMax   0.001  19.2% +- 2.8\n","1     Base   0.010  17.4% +- 3.5\n","0     Base   0.001  10.0% +- 2.9"],"text/html":["\n","  <div id=\"df-ccdf2696-4c8c-433c-8765-59f2a1834dcb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MODEL</th>\n","      <th>LR</th>\n","      <th>ACC_STD</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>SoftMax</td>\n","      <td>10.000</td>\n","      <td>94.2% +- 0.1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Base</td>\n","      <td>10.000</td>\n","      <td>93.4% +- 0.2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>SoftMax</td>\n","      <td>1.000</td>\n","      <td>93.2% +- 0.2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Base</td>\n","      <td>1.000</td>\n","      <td>88.8% +- 0.5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>SoftMax</td>\n","      <td>0.100</td>\n","      <td>87.1% +- 0.3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>SoftMax</td>\n","      <td>0.010</td>\n","      <td>67.0% +- 1.3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Base</td>\n","      <td>0.100</td>\n","      <td>54.1% +- 6.1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>SoftMax</td>\n","      <td>0.001</td>\n","      <td>19.2% +- 2.8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Base</td>\n","      <td>0.010</td>\n","      <td>17.4% +- 3.5</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Base</td>\n","      <td>0.001</td>\n","      <td>10.0% +- 2.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccdf2696-4c8c-433c-8765-59f2a1834dcb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ccdf2696-4c8c-433c-8765-59f2a1834dcb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ccdf2696-4c8c-433c-8765-59f2a1834dcb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-6defe060-640e-4c70-ba80-5b70fd240b9d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6defe060-640e-4c70-ba80-5b70fd240b9d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-6defe060-640e-4c70-ba80-5b70fd240b9d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \")\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"MODEL\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Base\",\n          \"SoftMax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.11823146508304,\n        \"min\": 0.001,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.0,\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ACC_STD\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"17.4% +- 3.5\",\n          \"93.4% +- 0.2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}],"source":["class Task1(Network):\n","    def __init__(self, sizes: List[int]):\n","        # initialize biases and weights with random normal distr.\n","        super().__init__(sizes=sizes)\n","\n","    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n","        # Run the network on a single case\n","\n","        ## TODO\n","        ###{\n","        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n","            a = sigmoid(a @ w + b)\n","\n","        return stable_softmax(a @ self.weights[-1] + self.biases[-1])\n","        ###}\n","\n","        return a\n","\n","    def backprop(\n","        self, x: NDArray[float], y: NDArray[float]\n","    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","\n","        assert len(x.shape) == 2  # batch, features\n","        assert len(y.shape) == 2  # batch, classes\n","        assert x.shape[0] == y.shape[0]\n","\n","        ##TODO\n","        ###{\n","        assert len(x.shape) == 2  # batch, features\n","        assert len(y.shape) == 2  # batch, classes\n","        assert x.shape[0] == y.shape[0]\n","\n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = []\n","        delta_nabla_w = []\n","\n","        # Then go forward remembering each layer input and value\n","        # before sigmoid activation\n","        layer_input = []\n","        before_act = []\n","        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n","            layer_input.append(x)\n","            x = x @ w + b\n","            before_act.append(x)\n","            x = sigmoid(x)\n","        layer_input.append(x)\n","        x = x @ self.weights[-1] + self.biases[-1]\n","        before_act.append(x)\n","        x = stable_softmax(x)\n","\n","        # Now go backward from the final cost applying backpropagation\n","        diff = self.cost_derivative(output_activations=x, y=y)\n","        delta_nabla_b.append(np.sum(diff, axis=0))\n","        delta_nabla_w.append(layer_input[-1].T @ diff)\n","        diff = diff @ self.weights[-1].T\n","        for linp, bef_act, w, b in reversed(\n","            list(zip(layer_input[:-1], before_act[:-1], self.weights[:-1], self.biases[:-1]))\n","        ):\n","            diff = sigmoid_prime(bef_act) * diff\n","            delta_nabla_w.append(linp.T @ diff)\n","            delta_nabla_b.append(np.sum(diff, axis=0))\n","            diff = diff @ w.T\n","\n","        delta_nabla_w = reversed(delta_nabla_w)\n","        delta_nabla_b = reversed(delta_nabla_b)\n","        ###}\n","\n","        # Check shapes\n","        delta_nabla_b = list(delta_nabla_b)\n","        delta_nabla_w = list(delta_nabla_w)\n","        assert len(delta_nabla_b) == len(self.biases), (\n","            len(delta_nabla_b),\n","            len(self.biases),\n","        )\n","        assert len(delta_nabla_w) == len(self.weights), (\n","            len(delta_nabla_w),\n","            len(self.weights),\n","        )\n","        for lid in range(len(self.weights)):\n","            assert delta_nabla_b[lid].shape == self.biases[lid].shape, (\n","                delta_nabla_b[lid].shape,\n","                self.biases[lid].shape,\n","            )\n","            assert delta_nabla_w[lid].shape == self.weights[lid].shape, (\n","                delta_nabla_w[lid].shape,\n","                self.weights[lid].shape,\n","            )\n","\n","        return delta_nabla_b, delta_nabla_w\n","\n","    def cost_derivative(\n","        self, output_activations: NDArray[float], y: NDArray[float]\n","    ) -> NDArray[float]:\n","        assert output_activations.shape == y.shape, (output_activations.shape, y.shape)\n","        ## TODO\n","        ###{\n","        return (output_activations - y) / len(y)  # moved here from update_mini_batch\n","        ###}\n","\n","\n","evaluate_model(\n","    model_name=\"SoftMax\",\n","    model_constructor=lambda x: Task1(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"markdown","metadata":{"id":"sXpjuhRa1qoR"},"source":["## Task 2\n","Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n","A few notes:\n","* do not regularize the biases\n","* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"]},{"cell_type":"code","source":["np.random.normal(0, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgiakrRrCu5F","executionInfo":{"status":"ok","timestamp":1730889470340,"user_tz":-60,"elapsed":399,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}},"outputId":"60e016bc-d201-43e9-c8ac-fa69c4388824"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5864285016590159"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M92vED2I1qoS","colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"status":"error","timestamp":1730889618116,"user_tz":-60,"elapsed":82043,"user":{"displayName":"Kacper Omieliańczyk","userId":"08113122392382505022"}},"outputId":"a2820db2-8f6d-4a31-d1e0-67e7d9f168be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.001\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 9, Accuracy: 0.0974: 100%|██████████| 10/10 [00:23<00:00,  2.35s/it]\n","Epoch: 9, Accuracy: 0.0974: 100%|██████████| 10/10 [00:20<00:00,  2.00s/it]\n","Epoch: 9, Accuracy: 0.0974: 100%|██████████| 10/10 [00:22<00:00,  2.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checking with lr = 0.01\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 5, Accuracy: 0.0974:  60%|██████    | 6/10 [00:14<00:09,  2.35s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-afdecda13889>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m evaluate_model(\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L2&Momentum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTask2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-1b2d7c87f7fc>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model_name, model_constructor, result_storage, layers)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             accuracy = network.optimize(\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-1b2d7c87f7fc>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 ]\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 epoch_bar.set_description_str(\n","\u001b[0;32m<ipython-input-17-afdecda13889>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, x_mini_batch, y_mini_batch, eta)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m## TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m###{\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-1b2d7c87f7fc>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mlayer_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mbefore_act\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["class Task2(Network):\n","    def __init__(\n","        self, sizes: List[int], l2_factor: float = 1e-4, momentum: float = 0.05\n","    ):\n","        # initialize biases and weights with random normal distr.\n","        super().__init__(sizes=sizes)\n","        self.l2_factor = l2_factor\n","        self.momentum = momentum\n","        ## TODO\n","        ####{\n","        ###}\n","\n","    def update_mini_batch(\n","        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n","    ) -> None:\n","        # Update network weights and biases by applying a single step\n","        # of gradient descent (with momentum and l2 regularization) using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        ## TODO\n","        ###{\n","\n","        ###}\n","\n","\n","evaluate_model(\n","    model_name=\"L2&Momentum\",\n","    model_constructor=lambda x: Task2(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"markdown","metadata":{"id":"uITexbde1qoU"},"source":["## Task 3 (optional)\n","Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts etc.). Again, test to see how these changes improve accuracy/convergence.  \n","In case you want to learn about AdamW you can check the official [PyTorch docummentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) that contains pseudocode and the original paper [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).  \n","In Appendix B you can find a simplified version of AdaGrad.  \n","Below you can find brief information regarding the dropout:  \n","\n","During the training phase, we want to make some activations $0$.\n","It is usually implemented by zeroing each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYXN22zx1qoU"},"outputs":[],"source":["class Task3Optimizer(Network):\n","    def __init__(\n","        self,\n","        sizes: List[int],\n","        weight_decay: float = 0.01,\n","        beta_1: float = 0.9,\n","        beta_2: float = 0.95,\n","        eps=1e-5,\n","    ):\n","        # initialize biases and weights with random normal distr.\n","        super().__init__(sizes=sizes)\n","        self.weight_decay = weight_decay\n","        self.beta_1 = beta_1\n","        self.beta_2 = beta_2\n","        self.eps = eps\n","        ## TODO\n","        ####{\n","        pass\n","        ###}\n","\n","    def update_mini_batch(\n","        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n","    ) -> None:\n","        # Update network weights and biases by applying a single step\n","        # of gradient descent (with momentum and weight decay) using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        ## TODO\n","        ###{\n","        pass\n","        ###}\n","\n","\n","evaluate_model(\n","    model_name=\"Optimizer\",\n","    model_constructor=lambda x: Task3Optimizer(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kp71bl1a1qoV"},"outputs":[],"source":["# Place for remaining parts of task 3"]},{"cell_type":"markdown","metadata":{"id":"RguR73jU1qoV"},"source":["## Task 4\n","Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0k4OAUuY1qoW"},"outputs":[],"source":["## TODO"]},{"cell_type":"markdown","metadata":{"id":"DCBNoRtH1qoW"},"source":["# Appendix"]},{"cell_type":"markdown","metadata":{"id":"jDyDrnMU1qoW"},"source":["## Appendix A - Log-loss and softmax\n","\n","Let's compute the following derivative\n","$$\\frac{\\partial\\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}}$$\n","\n","\n","where\n","$$\n","y_{b, i} = \\begin{cases}\n","1 & \\text{if $b$'th image belongs to $i$'th class}\\\\\n","0 & \\text{otherwise}\n","\\end{cases}\n","$$\n","\n","\n","To do so we can use the chain rule:\n","$$\n","\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)}  \\frac{\\partial g(x)}{\\partial x}\n","$$\n","the rule for the sum  \n","$$\n","\\frac{\\partial (f(x) + g(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x} +   \\frac{\\partial g(x)}{\\partial x}\n","$$\n","and the fact that the derivative of natural logarithm is\n","$$\n","\\frac{\\partial \\log(x)}{\\partial x} = \\frac{1}{x}\n","$$\n","\n","Using those rules we can observe that indeed:\n","\n","$$\n","\\frac{\\partial  \\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}} = \\sum_{i}{y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}}}\n","$$\n","\n","\n","\n","Let us review the multiplication rule:\n","$$\n","\\frac{\\partial (f(x)g(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x}g(x)  + f(x)\\frac{\\partial g(x)}{\\partial x}\n","$$\n","and that\n","$$\n","\\frac{\\partial e^x}{\\partial x} = e^x\n","$$\n","\n","From the definition:\n","$$\n","\\text{prediction}_{b, f} = \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\n","$$\n","\n","\n","We can observe that for $k \\not= f$\n","$$\n","\\frac{ \\partial \\text{prediction}_{b, k}}{\\partial z_{b, f}}\n","= 0 - \\frac{e^{z_{k, b}} e^{z_{b, f}} }{\\left(\\sum_{i}{e^{z_{b, i}}}\\right)^2}\n","$$\n","and that\n","$$\n","\\frac{ \\partial \\text{prediction}_{b, f}}{\\partial z_{b, f}}\n","= \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}} - \\left(\\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right)^2\n","$$\n","\n","Therefore\n","$$\n","y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}} = \\begin{cases}\n","y_{b,i}\\left(-\\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right) & \\text{if } i \\not= f\\\\\n","y_{b,i}\\left(1 - \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right) & \\text{if } i = f\\\\\n","\\end{cases}\n","$$\n","in other words\n","$$\n","y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}} = \\begin{cases}\n","y_{b,i}\\left(-\\text{prediction}_{b, f}\\right) & \\text{if } i \\not= f\\\\\n","y_{b,i}\\left(1 - \\text{prediction}_{b, f}\\right) & \\text{if } i = f\\\\\n","\\end{cases}\n","$$\n","\n","As we just sum over $i$, and for each $b$ there is exactly one $i$ such that $y_{b, i} = 1$ and for $j \\not= i$ $y_{b, j} = 0$, therefore  in the end we have that\n","$$\n","\\frac{\\partial  \\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}} = y_{b, f} - \\text{prediction}_{b, f}\n","$$\n","what can be written as:\n","$$\n","y - prediction\n","$$\n","\n","In the code, we are going to use\n","$$\n","prediction - y\n","$$\n","as our loss is\n","$$-\\sum_{b}\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}$$"]},{"cell_type":"markdown","metadata":{"id":"9kiBT-tJ1qoX"},"source":["## Appendix B - Adagrad (simplified version)\n","\n","Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n","For parameter $p_i$ we maintain an array $G_i$ (can be set to $0$ initially).\n","Let $\\text{LOSS}$ be our loss without L2.   \n","We update $G_i$ and $p_i$ each training step as follows:  \n","$$\n","G_i = G_i +  \\left(\\frac{\\partial \\text{LOSS}}{\\partial p_i}\\right)^2\\\\\n","p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\text{LOSS}}{\\partial p_i}\n","$$"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mim-ml-teaching/public-dnn-2024-25/blob/master/docs/DNN-Lab-3-mnist-again-student.ipynb","timestamp":1730315469142},{"file_id":"1mbRybTuEd5hfMgPq47jAy40aizNX03x8","timestamp":1665425459792},{"file_id":"1hs2ViNkY7vFE7l_PL7b-2XIN17cxbsyL","timestamp":1635823858600},{"file_id":"1t76la2tUWVLnEK7IKxdFZn_Y0be3xZud","timestamp":1635823610862}]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}